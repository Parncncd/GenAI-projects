{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "ebjOvuHGsk9e"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain"
      ],
      "metadata": {
        "id": "-DBLJWkHaCSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet langchain langchain_community langchain-anthropic langchain_experimental pypdf huggingface_hub sentence-transformers chromadb docarray"
      ],
      "metadata": {
        "id": "m3q_4LsOo-mI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ],
      "metadata": {
        "id": "qJ2Segphl6P4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# Load PDF\n",
        "loaders = [\n",
        "    # Duplicate documents on purpose - messy data\n",
        "    PyPDFLoader(\"https://see.stanford.edu/materials/aimlcs229/transcripts/MachineLearning-Lecture01.pdf\"),\n",
        "    PyPDFLoader(\"https://see.stanford.edu/materials/aimlcs229/transcripts/MachineLearning-Lecture02.pdf\"),\n",
        "    PyPDFLoader(\"https://see.stanford.edu/materials/aimlcs229/transcripts/MachineLearning-Lecture03.pdf\"),\n",
        "    PyPDFLoader(\"https://see.stanford.edu/materials/aimlcs229/transcripts/MachineLearning-Lecture04.pdf\"),\n",
        "]\n",
        "docs = []\n",
        "for loader in loaders:\n",
        "    docs.extend(loader.load())"
      ],
      "metadata": {
        "id": "RoZp6pBPcA0y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting"
      ],
      "metadata": {
        "id": "hdlnx_T-mBr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1500,\n",
        "    chunk_overlap = 150\n",
        ")\n",
        "\n",
        "splits = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "_YrokW8HfP3i"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(splits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRAAmO4nEEYg",
        "outputId": "8b0e91ae-0b96-487d-f49f-653b021f66aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "199"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VectorStore & Embedding"
      ],
      "metadata": {
        "id": "EIWJZNsPoAPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings.sentence_transformer import (\n",
        "    SentenceTransformerEmbeddings,\n",
        ")\n",
        "\n",
        "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "\n",
        "# save to disk\n",
        "db1 = Chroma.from_documents(docs, embedding_function, persist_directory=\"./chroma_db\")\n",
        "\n",
        "# load from disk\n",
        "db2 = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_function)\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "EY6RmgELnP0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# DEFAULT_SENTENCE_EMBEDDING_MODEL = 'intfloat/multilingual-e5-base'\n",
        "SENTENCE_EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n",
        "\n",
        "persist_directory = 'chromadb'\n",
        "\n",
        "chromadb = Chroma.from_documents(\n",
        "    docs,\n",
        "    embedding = HuggingFaceEmbeddings(model_name=SENTENCE_EMBEDDING_MODEL),\n",
        "    persist_directory=persist_directory\n",
        ")"
      ],
      "metadata": {
        "id": "tfDXV0qjpOVm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval"
      ],
      "metadata": {
        "id": "4jnFvK7OnwuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similarity Search"
      ],
      "metadata": {
        "id": "GehXm_h3ocGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what did they say about matlab?\"\n",
        "docs_ss = chromadb.similarity_search(question,k=3)"
      ],
      "metadata": {
        "id": "99s_X65tm5zI"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'1: {docs_ss[0].page_content[:100]}')\n",
        "print(f'2: {docs_ss[1].page_content[:100]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUNYFrANnoXG",
        "outputId": "365bf0d7-8a40-4c3d-b9fc-4d80cb6ae35d"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: those homeworks will be done in either MATLA B or in Octave, which is sort of — I \n",
            "know some people \n",
            "2: those homeworks will be done in either MATLA B or in Octave, which is sort of — I \n",
            "know some people \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Max Marginal Relevance search (MMR)\n",
        "\n",
        "The idea of MMR is that you may not always want to choose the most similar responses.\n",
        "Maximum marginal relevance strives to achieve both relevance to the query and diversity among the results."
      ],
      "metadata": {
        "id": "Ddn4dciHofJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs_mmr = chromadb.max_marginal_relevance_search(question,k=3)"
      ],
      "metadata": {
        "id": "ACRDRBMhnC1W"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'1: {docs_mmr[0].page_content[:100]}')\n",
        "print(f'2: {docs_mmr[1].page_content[:100]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-hSaf6InIfX",
        "outputId": "17c01ed1-58ea-467f-975d-ee72a42489a4"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: those homeworks will be done in either MATLA B or in Octave, which is sort of — I \n",
            "know some people \n",
            "2: And as an aside, this algorithm I just showed you, it seems like it must be a pretty \n",
            "complicated al\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what did they say about regression in the third lecture?\"\n",
        "docs = chromadb.similarity_search(question, k=3)\n",
        "\n",
        "for doc in docs:\n",
        "    print(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:300]+'\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCAU18-NuChX",
        "outputId": "a462d89c-8b85-423c-8765-75ca1b95d0cf"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: MachineLearning-Lecture03  \n",
            "Instructor (Andrew Ng) :Okay. Good morning and welcome b ack to the third lecture of \n",
            "this class. So here’s what I want to do t oday, and some of the topics I do today may seem \n",
            "a little bit like I’m jumping, sort  of, from topic to topic, but here’s, sort of, the outline\n",
            "\n",
            "0: MachineLearning-Lecture03  \n",
            "Instructor (Andrew Ng) :Okay. Good morning and welcome b ack to the third lecture of \n",
            "this class. So here’s what I want to do t oday, and some of the topics I do today may seem \n",
            "a little bit like I’m jumping, sort  of, from topic to topic, but here’s, sort of, the outline\n",
            "\n",
            "2: Instructor (Andrew Ng) :All right, so who thought driving could be that dramatic, right? \n",
            "Switch back to the chalkboard, please. I s hould say, this work was done about 15 years \n",
            "ago and autonomous driving has come a long way. So many of you will have heard of the \n",
            "DARPA Grand Challenge, where one o\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in docs:\n",
        "    print(doc.metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhOVSyDnkyDi",
        "outputId": "df5e2958-f745-43ef-dac7-204dc8436233"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'page': 0, 'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/MachineLearning-Lecture03.pdf'}\n",
            "{'page': 0, 'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/MachineLearning-Lecture03.pdf'}\n",
            "{'page': 2, 'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/MachineLearning-Lecture02.pdf'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other types of retrieval"
      ],
      "metadata": {
        "id": "ebjOvuHGsk9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import SVMRetriever\n",
        "from langchain.retrievers import TFIDFRetriever\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "DdnVUCzjso5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Load PDF\n",
        "loader = PyPDFLoader(\"https://see.stanford.edu/materials/aimlcs229/transcripts/MachineLearning-Lecture01.pdf\")\n",
        "pages = loader.load()\n",
        "all_page_text=[p.page_content for p in pages]\n",
        "joined_page_text=\" \".join(all_page_text)\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1500,chunk_overlap = 150)\n",
        "splits = text_splitter.split_text(joined_page_text)\n",
        "\n",
        "# Retrieve\n",
        "svm_retriever = SVMRetriever.from_texts(splits,embedding)\n",
        "tfidf_retriever = TFIDFRetriever.from_texts(splits)"
      ],
      "metadata": {
        "id": "nTIqsdgis0vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What are major topics for this class?\"\n",
        "docs_svm=svm_retriever.get_relevant_documents(question)\n",
        "docs_svm[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObZVJIkmt22C",
        "outputId": "31fc960f-bae1-4bb2-abf0-e3cacfaaa743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content=\"let me just check what questions you have righ t now. So if there are no questions, I'll just \\nclose with two reminders, which are after class today or as you start to talk with other \\npeople in this class, I just encourage you again to start to form project partners, to try to \\nfind project partners to do your project with. And also, this is a good time to start forming \\nstudy groups, so either talk to your friends  or post in the newsgroup, but we just \\nencourage you to try to star t to do both of those today, okay? Form study groups, and try \\nto find two other project partners.  \\nSo thank you. I'm looking forward to teaching this class, and I'll see you in a couple of \\ndays.   [End of Audio]  \\nDuration: 69 minutes\")"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what did they say about matlab?\"\n",
        "docs_tfidf=tfidf_retriever.get_relevant_documents(question)\n",
        "docs_tfidf[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_l4hXOxmt4A9",
        "outputId": "5723a468-eee3-4322-86e0-bb3e509502d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content=\"Saxena and Min Sun here did, wh ich is given an image like this, right? This is actually a \\npicture taken of the Stanford campus. You can apply that sort of cl ustering algorithm and \\ngroup the picture into regions. Let me actually blow that up so that you can see it more \\nclearly. Okay. So in the middle, you see the lines sort of groupi ng the image together, \\ngrouping the image into [inaudible] regions.  \\nAnd what Ashutosh and Min did was they then  applied the learning algorithm to say can \\nwe take this clustering and us e it to build a 3D model of the world? And so using the \\nclustering, they then had a lear ning algorithm try to learn what the 3D structure of the \\nworld looks like so that they could come up with a 3D model that you can sort of fly \\nthrough, okay? Although many people used to th ink it's not possible to take a single \\nimage and build a 3D model, but using a lear ning algorithm and that sort of clustering \\nalgorithm is the first step. They were able to.  \\nI'll just show you one more example. I like this  because it's a picture of Stanford with our \\nbeautiful Stanford campus. So again, taking th e same sort of clustering algorithms, taking \\nthe same sort of unsupervised learning algor ithm, you can group the pixels into different \\nregions. And using that as a pre-processing step, they eventually built this sort of 3D model of Stanford campus in a single picture.  You can sort of walk  into the ceiling, look\")"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question-Answering"
      ],
      "metadata": {
        "id": "nhpafKqPuyh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Claude3"
      ],
      "metadata": {
        "id": "zgooEsbWiNpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# set key to ENV\n",
        "key = 'ANTHROPIC_API_KEY'\n",
        "os.environ[key] = userdata.get(key)"
      ],
      "metadata": {
        "id": "QfSUBlb9jVt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "chat = ChatAnthropic(temperature=0, model_name=\"claude-3-opus-20240229\")"
      ],
      "metadata": {
        "id": "609dRjIPmj6l"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RetrievalQA chain"
      ],
      "metadata": {
        "id": "3MA_0g73lWfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RetrievalQA chain\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    chat,\n",
        "    retriever=chromadb.as_retriever(),\n",
        ")"
      ],
      "metadata": {
        "id": "ZSYSRqJ6_Kl2"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Is probability a class topic?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "PXGFFBJiFL9V",
        "outputId": "3495a9e1-4776-4ac2-c0fc-1037e5263595"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Yes, probability is an important prerequisite topic for this machine learning class. The instructor mentions that he assumes students have familiarity with basic probability and statistics, such as knowing what random variables, expectation, and variance are. \\n\\nHe says most undergraduate statistics classes, like Stat 116 at Stanford, will provide more than enough probability background for this course. He also mentions that for students who haven't seen probability material in a while, some of the discussion sections will review the probability prerequisites as a refresher.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt"
      ],
      "metadata": {
        "id": "vk6o-YWvlf8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Build prompt\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n"
      ],
      "metadata": {
        "id": "7rBh2mRhmwaK"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RetrievalQA chain\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    chat,\n",
        "    retriever=chromadb.as_retriever(),\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")"
      ],
      "metadata": {
        "id": "cV6VuH_NmrN1"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Is probability a class topic?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "1paKZwPbmtbb",
        "outputId": "00b42488-cf8b-42ff-e579-273f3eaa5646"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, probability is a topic covered in this class. The instructor mentions that if students need a refresher on the foundations of probability, the discussion sections taught by the TAs will review probability. Thanks for asking!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo"
      ],
      "metadata": {
        "id": "ouf09E9RpPAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU gradio unidecode accelerate"
      ],
      "metadata": {
        "id": "vBsP_7R2pSH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "58sKr-Wis8OJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "source https://huggingface.co/spaces/cvachet/pdf-chatbot"
      ],
      "metadata": {
        "id": "v4ubFzarzZUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "from getpass import getpass\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "from pathlib import Path\n",
        "import chromadb\n",
        "from unidecode import unidecode\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "import tqdm\n",
        "import accelerate\n",
        "import re\n",
        "\n",
        "# Load PDF document and create doc splits\n",
        "def load_doc(list_file_path, chunk_size, chunk_overlap):\n",
        "    # Processing for one document only\n",
        "    loaders = [PyPDFLoader(x) for x in list_file_path]\n",
        "    pages = []\n",
        "    for loader in loaders:\n",
        "        pages.extend(loader.load())\n",
        "    # text_splitter = RecursiveCharacterTextSplitter(chunk_size = 600, chunk_overlap = 50)\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size = chunk_size,\n",
        "        chunk_overlap = chunk_overlap)\n",
        "    doc_splits = text_splitter.split_documents(pages)\n",
        "    return doc_splits\n",
        "\n",
        "# Create vector database\n",
        "def create_db(splits, collection_name):\n",
        "    embedding = HuggingFaceEmbeddings()\n",
        "    new_client = chromadb.EphemeralClient()\n",
        "    vectordb = Chroma.from_documents(\n",
        "        documents=splits,\n",
        "        embedding=embedding,\n",
        "        client=new_client,\n",
        "        collection_name=collection_name,\n",
        "    )\n",
        "    return vectordb\n",
        "\n",
        "\n",
        "# Load vector database\n",
        "def load_db():\n",
        "    embedding = HuggingFaceEmbeddings()\n",
        "    vectordb = Chroma(\n",
        "        embedding_function=embedding)\n",
        "    return vectordb\n",
        "\n",
        "\n",
        "# Initialize langchain LLM chain\n",
        "def initialize_llmchain(key, temperature, max_tokens, top_k, vector_db, progress=gr.Progress()):\n",
        "    progress(0.1, desc=\"Initializing...\")\n",
        "\n",
        "    llm = ChatAnthropic(model_name=\"claude-3-opus-20240229\",\n",
        "                        temperature=temperature,\n",
        "                        anthropic_api_key=key\n",
        "                        # max_new_tokens = max_tokens,\n",
        "                        # top_k = top_k,\n",
        "                        )\n",
        "\n",
        "    progress(0.75, desc=\"Defining buffer memory...\")\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key=\"chat_history\",\n",
        "        output_key='answer',\n",
        "        return_messages=True\n",
        "    )\n",
        "    # retriever=vector_db.as_retriever(search_type=\"similarity\", search_kwargs={'k': 3})\n",
        "    retriever=vector_db.as_retriever()\n",
        "    progress(0.8, desc=\"Defining retrieval chain...\")\n",
        "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm,\n",
        "        retriever=retriever,\n",
        "        chain_type=\"stuff\",\n",
        "        memory=memory,\n",
        "        # combine_docs_chain_kwargs={\"prompt\": your_prompt})\n",
        "        return_source_documents=True,\n",
        "        #return_generated_question=False,\n",
        "        verbose=False,\n",
        "    )\n",
        "    progress(0.9, desc=\"Done!\")\n",
        "    return qa_chain\n",
        "\n",
        "\n",
        "# Generate collection name for vector database\n",
        "#  - Use filepath as input, ensuring unicode text\n",
        "def create_collection_name(filepath):\n",
        "    # Extract filename without extension\n",
        "    collection_name = Path(filepath).stem\n",
        "    # Fix potential issues from naming convention\n",
        "    ## Remove space\n",
        "    collection_name = collection_name.replace(\" \",\"-\")\n",
        "    ## ASCII transliterations of Unicode text\n",
        "    collection_name = unidecode(collection_name)\n",
        "    ## Remove special characters\n",
        "    #collection_name = re.findall(\"[\\dA-Za-z]*\", collection_name)[0]\n",
        "    collection_name = re.sub('[^A-Za-z0-9]+', '-', collection_name)\n",
        "    ## Limit length to 50 characters\n",
        "    collection_name = collection_name[:50]\n",
        "    ## Minimum length of 3 characters\n",
        "    if len(collection_name) < 3:\n",
        "        collection_name = collection_name + 'xyz'\n",
        "    ## Enforce start and end as alphanumeric character\n",
        "    if not collection_name[0].isalnum():\n",
        "        collection_name = 'A' + collection_name[1:]\n",
        "    if not collection_name[-1].isalnum():\n",
        "        collection_name = collection_name[:-1] + 'Z'\n",
        "    print('Filepath: ', filepath)\n",
        "    print('Collection name: ', collection_name)\n",
        "    return collection_name\n",
        "\n",
        "\n",
        "# Initialize database\n",
        "def initialize_database(list_file_obj, chunk_size, chunk_overlap, progress=gr.Progress()):\n",
        "    # Create list of documents (when valid)\n",
        "    list_file_path = [x.name for x in list_file_obj if x is not None]\n",
        "    # Create collection_name for vector database\n",
        "    progress(0.1, desc=\"Creating collection name...\")\n",
        "    collection_name = create_collection_name(list_file_path[0])\n",
        "    progress(0.25, desc=\"Loading document...\")\n",
        "    # Load document and create splits\n",
        "    doc_splits = load_doc(list_file_path, chunk_size, chunk_overlap)\n",
        "    # Create or load vector database\n",
        "    progress(0.5, desc=\"Generating vector database...\")\n",
        "    # global vector_db\n",
        "    vector_db = create_db(doc_splits, collection_name)\n",
        "    progress(0.9, desc=\"Done!\")\n",
        "    return vector_db, collection_name, \"Complete!\"\n",
        "\n",
        "\n",
        "def initialize_LLM( key, llm_temperature, max_tokens, top_k, vector_db, progress=gr.Progress()):\n",
        "    qa_chain = initialize_llmchain( key, llm_temperature, max_tokens, top_k, vector_db, progress)\n",
        "    return qa_chain, \"Complete!\"\n",
        "\n",
        "\n",
        "def format_chat_history(message, chat_history):\n",
        "    formatted_chat_history = []\n",
        "    for user_message, bot_message in chat_history:\n",
        "        formatted_chat_history.append(f\"User: {user_message}\")\n",
        "        formatted_chat_history.append(f\"Assistant: {bot_message}\")\n",
        "    return formatted_chat_history\n",
        "\n",
        "\n",
        "def conversation(qa_chain, message, history):\n",
        "    formatted_chat_history = format_chat_history(message, history)\n",
        "\n",
        "    # Generate response using QA chain\n",
        "    response = qa_chain({\"question\": message, \"chat_history\": formatted_chat_history})\n",
        "    response_answer = response[\"answer\"]\n",
        "    if response_answer.find(\"Helpful Answer:\") != -1:\n",
        "        response_answer = response_answer.split(\"Helpful Answer:\")[-1]\n",
        "    response_sources = response[\"source_documents\"]\n",
        "    response_source1 = response_sources[0].page_content.strip()\n",
        "    response_source2 = response_sources[1].page_content.strip()\n",
        "    response_source3 = response_sources[2].page_content.strip()\n",
        "\n",
        "    # Langchain sources are zero-based\n",
        "    response_source1_page = response_sources[0].metadata[\"page\"] + 1\n",
        "    response_source2_page = response_sources[1].metadata[\"page\"] + 1\n",
        "    response_source3_page = response_sources[2].metadata[\"page\"] + 1\n",
        "\n",
        "    # Append user message and response to chat history\n",
        "    new_history = history + [(message, response_answer)]\n",
        "    return qa_chain, gr.update(value=\"\"), new_history, response_source1, response_source1_page, response_source2, response_source2_page, response_source3, response_source3_page\n",
        "\n",
        "\n",
        "def upload_file(file_obj):\n",
        "    list_file_path = []\n",
        "    for idx, file in enumerate(file_obj):\n",
        "        file_path = file_obj.name\n",
        "        list_file_path.append(file_path)\n",
        "    return list_file_path\n",
        "\n",
        "\n",
        "def demo():\n",
        "    with gr.Blocks(theme=\"base\") as demo:\n",
        "        vector_db = gr.State()\n",
        "        qa_chain = gr.State()\n",
        "        collection_name = gr.State()\n",
        "\n",
        "        gr.Markdown(\n",
        "        \"\"\"<center><h2>PDF-based chatbot (powered by LangChain and Anthropic Claude-3)</center></h2>\n",
        "        <h3>Ask any questions about your PDF documents, along with follow-ups</h3>\n",
        "        <b>Note:</b> This AI assistant performs retrieval-augmented generation from your PDF documents. \\\n",
        "        When generating answers, it takes past questions into account (via conversational memory), and includes document references for clarity purposes.</i>\n",
        "        <br><b>Warning:</b> This space uses the free CPU Basic hardware from Hugging Face. Some steps and LLM models used below (free inference endpoints) can take some time to generate an output.<br>\n",
        "        \"\"\")\n",
        "        with gr.Tab(\"Step 1 - Document pre-processing\"):\n",
        "            with gr.Row():\n",
        "                document = gr.Files(height=100, file_count=\"multiple\", file_types=[\"pdf\"], interactive=True, label=\"Upload your PDF documents (single or multiple)\")\n",
        "                # upload_btn = gr.UploadButton(\"Loading document...\", height=100, file_count=\"multiple\", file_types=[\"pdf\"], scale=1)\n",
        "            with gr.Row():\n",
        "                db_btn = gr.Radio([\"ChromaDB\"], label=\"Vector database type\", value = \"ChromaDB\", type=\"index\", info=\"Choose your vector database\")\n",
        "            with gr.Accordion(\"Advanced options - Document text splitter\", open=False):\n",
        "                with gr.Row():\n",
        "                    slider_chunk_size = gr.Slider(minimum = 100, maximum = 1000, value=600, step=20, label=\"Chunk size\", info=\"Chunk size\", interactive=True)\n",
        "                with gr.Row():\n",
        "                    slider_chunk_overlap = gr.Slider(minimum = 10, maximum = 200, value=40, step=10, label=\"Chunk overlap\", info=\"Chunk overlap\", interactive=True)\n",
        "            with gr.Row():\n",
        "                db_progress = gr.Textbox(label=\"Vector database initialization\", value=\"None\")\n",
        "            with gr.Row():\n",
        "                db_btn = gr.Button(\"Generate vector database...\")\n",
        "\n",
        "        with gr.Tab(\"Step 2 - Claude QA chain initialization\"):\n",
        "            with gr.Row():\n",
        "              gr.Markdown(\n",
        "                \"\"\"<h3>To use Anthropic models, you will need to set the ANTHROPIC_API_KEY environment variable. You can get an Anthropic API key <a href=\"https://console.anthropic.com/settings/keys\">here</a></h3>\"\"\")\n",
        "            with gr.Row():\n",
        "                claude_key = gr.Textbox(placeholder=\"Enter your Anthropic API Key...\", container=True,label=\"Anthropic API Key\")\n",
        "            with gr.Accordion(\"Advanced options - LLM model\", open=False):\n",
        "                with gr.Row():\n",
        "                    slider_temperature = gr.Slider(minimum = 0.0, maximum = 1.0, value=0.7, step=0.1, label=\"Temperature\", info=\"Model temperature\", interactive=True)\n",
        "                with gr.Row():\n",
        "                    slider_maxtokens = gr.Slider(minimum = 224, maximum = 4096, value=1024, step=32, label=\"Max Tokens\", info=\"Model max tokens\", interactive=True)\n",
        "                with gr.Row():\n",
        "                    slider_topk = gr.Slider(minimum = 1, maximum = 10, value=3, step=1, label=\"top-k samples\", info=\"Model top-k samples\", interactive=True)\n",
        "            with gr.Row():\n",
        "                llm_progress = gr.Textbox(value=\"None\",label=\"QA chain initialization\")\n",
        "            with gr.Row():\n",
        "                qachain_btn = gr.Button(\"Initialize question-answering chain...\")\n",
        "\n",
        "        with gr.Tab(\"Step 3 - Conversation with chatbot\"):\n",
        "            chatbot = gr.Chatbot(height=300)\n",
        "            with gr.Accordion(\"Advanced - Document references\", open=False):\n",
        "                with gr.Row():\n",
        "                    doc_source1 = gr.Textbox(label=\"Reference 1\", lines=2, container=True, scale=20)\n",
        "                    source1_page = gr.Number(label=\"Page\", scale=1)\n",
        "                with gr.Row():\n",
        "                    doc_source2 = gr.Textbox(label=\"Reference 2\", lines=2, container=True, scale=20)\n",
        "                    source2_page = gr.Number(label=\"Page\", scale=1)\n",
        "                with gr.Row():\n",
        "                    doc_source3 = gr.Textbox(label=\"Reference 3\", lines=2, container=True, scale=20)\n",
        "                    source3_page = gr.Number(label=\"Page\", scale=1)\n",
        "            with gr.Row():\n",
        "                msg = gr.Textbox(placeholder=\"Type message\", container=True)\n",
        "            with gr.Row():\n",
        "                submit_btn = gr.Button(\"Submit\")\n",
        "                clear_btn = gr.ClearButton([msg, chatbot])\n",
        "\n",
        "        # Preprocessing events\n",
        "        #upload_btn.upload(upload_file, inputs=[upload_btn], outputs=[document])\n",
        "        db_btn.click(initialize_database, \\\n",
        "            inputs=[document, slider_chunk_size, slider_chunk_overlap], \\\n",
        "            outputs=[vector_db, collection_name, db_progress])\n",
        "        qachain_btn.click(initialize_LLM, \\\n",
        "            inputs=[ claude_key, slider_temperature, slider_maxtokens, slider_topk, vector_db], \\\n",
        "            outputs=[qa_chain, llm_progress]).then(lambda:[None,\"\",0,\"\",0,\"\",0], \\\n",
        "            inputs=None, \\\n",
        "            outputs=[chatbot, doc_source1, source1_page, doc_source2, source2_page, doc_source3, source3_page], \\\n",
        "            queue=False)\n",
        "\n",
        "        # Chatbot events\n",
        "        msg.submit(conversation, \\\n",
        "            inputs=[qa_chain, msg, chatbot], \\\n",
        "            outputs=[qa_chain, msg, chatbot, doc_source1, source1_page, doc_source2, source2_page, doc_source3, source3_page], \\\n",
        "            queue=False)\n",
        "        submit_btn.click(conversation, \\\n",
        "            inputs=[qa_chain, msg, chatbot], \\\n",
        "            outputs=[qa_chain, msg, chatbot, doc_source1, source1_page, doc_source2, source2_page, doc_source3, source3_page], \\\n",
        "            queue=False)\n",
        "        clear_btn.click(lambda:[None,\"\",0,\"\",0,\"\",0], \\\n",
        "            inputs=None, \\\n",
        "            outputs=[chatbot, doc_source1, source1_page, doc_source2, source2_page, doc_source3, source3_page], \\\n",
        "            queue=False)\n",
        "    demo.queue().launch(debug=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "id": "vzWy9usSpMy2",
        "outputId": "bd7a9c56-b19b-4cee-c804-b4fbbcfdc907"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://fbe2019284f3abeb33.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://fbe2019284f3abeb33.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filepath:  /tmp/gradio/c4b6ce3f43b6e22b39e141f5ab30ad4303073505/MachineLearning-Lecture01.pdf\n",
            "Collection name:  MachineLearning-Lecture01\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://fbe2019284f3abeb33.gradio.live\n"
          ]
        }
      ]
    }
  ]
}